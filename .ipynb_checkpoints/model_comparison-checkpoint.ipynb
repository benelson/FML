{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Computing fully marginalized likelihoods for n-planet models </center>\n",
    "<img src=\"fml.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will compute the fully marginalized likelihood (i.e. FML, Bayesian evidence) for two specified models $\\mathcal{M}$ applied to radial velocity data $\\vec{d}$. The notebook will proceed in three phases:\n",
    "\n",
    "1. [generating a synthetic dataset](#section1) of a known system (two planets)\n",
    "2. [performing an MCMC](#section2) to get posterior samples of two competing models ([one planet](#section2.1) vs. [two planets](#section2.2))\n",
    "3. [computing FML](#section3) for each model to yield a Bayes factor that [hopefully](#section3.4) favors the input (i.e. two planets)\n",
    "\n",
    "We require a few modules for this to work. Some come with the standard Anaconda installation. You will have to $\\tt{pip\\,install}$ a few of these if you haven't already. I wrote several other functions specific to this project that can be categorized as \"creating\" things (e.g. formatting data in a certain way) and \"computing\" things (e.g. parameter transformations). These functions are located in $\\tt{create.py}$ and $\\tt{compute.py}$ respectively.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to experiment with the parameters here, especially the input system, observational model, and importance sampling parameters. If you decide to use the importance sampling algorithm described here for your own project, please cite [Nelson et al. 2016](http://adsabs.harvard.edu/abs/2016MNRAS.455.2484N).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# If you installed Python through Anaconda, you probably have these.\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "\n",
    "# You'll need to install these modules separately through e.g. pip.\n",
    "import emcee # for doing MCMC\n",
    "import corner # for multi-dimensional plotting\n",
    "import rebound # for doing the model evaluation\n",
    "\n",
    "# These .py files should reside in the same directory as the notebook.\n",
    "import compute\n",
    "import create\n",
    "import observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## <center> 1. Simulate radial velocity data for a known system.</center>\n",
    "\n",
    "We will generate a synthetic dataset from a two-planet model. These planets will be on approximately Keplerian (i.e. non-interacting) orbits around a 1 $M_\\odot$ star on the observing timescale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# central star mass\n",
    "mStar = 1.0\n",
    "\n",
    "# set orbital parameters for n-planets here\n",
    "params_true = {\"period\": [30., 61.],\\\n",
    "               \"half-amp\": [7., 30.],\\\n",
    "               \"eccentricity\": [0.2, 0.1],\\\n",
    "               \"argperi\": [np.pi/2., np.pi/3.],\\\n",
    "               \"meanomaly\": [np.pi/4., 0.]}\n",
    "\n",
    "# params_true_by_planet: list of lists that is essentially params_true transposed\n",
    "# params_true_chained: all parameters as 1-d list\n",
    "params_true_by_planet, params_true_chained = create.input_format(params_true)\n",
    "nPlanets_true = len(params_true_by_planet)\n",
    "\n",
    "# generate synthetic RV measurements\n",
    "obs = observations.FakeObservation(compute.sma_and_mass(params_true_chained, mStar),\\\n",
    "                                   nPoints=50, error=2.0, tmax=100., mStar=mStar)\n",
    "\n",
    "# plot the RV data\n",
    "plt.errorbar(obs.t, obs.rv, yerr=obs.err, fmt='.')\n",
    "plt.xlabel('Time [days]', fontsize=14)\n",
    "plt.ylabel('Radial velocity [m/s]', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section2'></a>\n",
    "## <center> 2. Do the MCMC </center>\n",
    "\n",
    "The FML algorithm in this notebook uses information from posterior samples, so we'll need to perform an MCMC on the dataset above for two competing models.\n",
    "\n",
    "First, we'll define our likelihood, prior, and posterior. We adopt uniform priors on $e$, $\\omega$, and $M$ from $\\{0,1\\}$, $\\{0,2\\pi\\}$, and $\\{0,2\\pi\\}$ respectively. We adopt modified Jeffreys priors for $p$ and $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lnlike(theta, obs):\n",
    "    '''\n",
    "    The log-likelihood function. Observational model assume RVs have independent Gaussian error bars.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : 1-d list of Keplerian orbital parameters (p, K, e, w, M)\n",
    "                for every modeled planet\n",
    "        obs   : observations object with radial velocity measurement (.rv)\n",
    "                and uncertainty (.err) attributes\n",
    "    '''\n",
    "    model = compute.rv(theta, obs)\n",
    "    inv_sigma2 = 1.0/(obs.err * obs.err)\n",
    "    return -0.5*( np.sum( (obs.rv - model)*(obs.rv - model)*inv_sigma2 - np.log(inv_sigma2) ) )\n",
    "\n",
    "def lnprior(theta):\n",
    "    '''\n",
    "    The log-prior function.\n",
    "        \n",
    "         Parameters\n",
    "         ----------\n",
    "         theta : 1-d list of Keplerian orbital parameters (p, K, e, w, M)\n",
    "                 for every modeled planet\n",
    "    '''\n",
    "    planets = [theta[x:x+5] for x in range(0, len(theta), 5)]\n",
    "    nPlanets = len(planets)\n",
    "    thetaT = np.transpose(planets)\n",
    "    p, K, e, w, M = thetaT[0], thetaT[1], thetaT[2], thetaT[3], thetaT[4]\n",
    "    \n",
    "    P0, K0 = 1., 1.\n",
    "    lnp = 0.\n",
    "    \n",
    "    for i in range(nPlanets):\n",
    "        if p[i] < 99999.and K[i] < 99999.and 0. <= e[i] < 1. \\\n",
    "        and 0. <= abs(w[i]) <= 2*np.pi and 0. <= abs(M[i]) <= 2*np.pi:\n",
    "            lnp += -np.log(1.+p[i]/P0) - np.log(1+K[i]/K0) - 2.*np.log(2.*np.pi)\n",
    "        else:\n",
    "            return -np.inf\n",
    "    return lnp\n",
    "    \n",
    "def lnpost(theta, obs):\n",
    "    '''\n",
    "    The log-posterior function. Sum of log-likelihood and log-prior.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : 1-d list of Keplerian orbital parameters (p, K, e, w, M)\n",
    "                for every modeled planet\n",
    "        obs   : observations object with radial velocity measurement (.rv)\n",
    "                and uncertainty (.err) attributes\n",
    "    '''\n",
    "    lp = lnprior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(theta, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MCMC needs a good starting condition in order to \"burn-in\" rapidly. In this notebook, we are only dealing with a handful of parameters (5-10) and fairly high signal-to-noise planets. So getting a maximum likelihood estimate (MLE) via e.g. scipy.optimize should work well enough.\n",
    "\n",
    "Initial conditions for the MCMC will be generated by creating a small dispersion around the MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as op\n",
    "\n",
    "def bounds(theta):\n",
    "    '''\n",
    "    Bounds function. Ensures gravitationally bound orbits (0 < e < 1) and some non-negative quantities.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : 1-d list of Keplerian orbital parameters (p, K, e, w, M)\n",
    "                for every modeled planet\n",
    "    '''\n",
    "    bnds = ()\n",
    "    for i in range(int(len(theta)/5)):\n",
    "        bnds = bnds + ((0., None), (0., None), (0.,1.), (-2.*np.pi, 2.*np.pi), (-2.*np.pi, 2.*np.pi))\n",
    "    return bnds\n",
    "\n",
    "def create_init(theta):\n",
    "    '''\n",
    "    Create initial conditions for MCMC based on input theta\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : 1-d list of Keplerian orbital parameters (p, K, e, w, M)\n",
    "                for every modeled planet\n",
    "    '''\n",
    "    nPlanets = int(len(theta)/5)\n",
    "    planets = [theta[x:x+5] for x in range(0, nPlanets*5, 5)]\n",
    "    init = []\n",
    "    for i, planet in enumerate(planets):\n",
    "        init.append(planet[0] + np.random.normal(0.,0.01))\n",
    "        init.append(planet[1] + np.random.normal(0.,0.05))\n",
    "        init.append(planet[2] + abs(np.random.normal(0.,0.01)))\n",
    "        init.append(planet[3] + np.random.normal(0.,0.5))\n",
    "        init.append(planet[4] + np.random.normal(0.,0.5))\n",
    "    return init\n",
    "\n",
    "nll = lambda *args: -lnlike(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider two competing models. One will be the true generative model (two-planets). The other will be one-planet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section2.1'></a>\n",
    "### <center>2.1. One-planet model </center>\n",
    "\n",
    "We'll use the properties of the higher signal-to-noise planet as a plausible one-planet model and get an MLE for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "system_1pl = params_true_by_planet[1]\n",
    "MLE_1pl = list( op.minimize( nll, system_1pl, args=obs, bounds=bounds(system_1pl) )['x'] )\n",
    "\n",
    "print(\"MLE for one-planet model           [p, K, e, w, M]:\")\n",
    "print(MLE_1pl)\n",
    "print(\"\")\n",
    "print(\"Actual properties from input model [p, K, e, w, M]:\")\n",
    "print(system_1pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the one-planet model, the orbital period, RV half-amplitude, and phase angles should be roughly consistent with the outer planet in the two-planet model, but the eccentricity here should be overestimated.\n",
    "\n",
    "---\n",
    "\n",
    "Now, let's do the MCMC to get constraints on each of these parameters. The runtime heavily depends on the system itself, the number/extent of observations, and the parameters set for the MCMC. The one-planet MCMC should only take a couple minutes for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nDim_1pl = len(MLE_1pl)\n",
    "nWalkers_1pl = 2*nDim_1pl\n",
    "init_1pl = [ create_init(MLE_1pl) for i in range(nWalkers_1pl) ]\n",
    "\n",
    "sampler_1pl = emcee.EnsembleSampler(nWalkers_1pl, nDim_1pl, lnpost, args=[obs])\n",
    "sampler_1pl.run_mcmc(init_1pl, 5500, thin=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_1pl = sampler_1pl.chain[:, 50:, :].reshape((-1, nDim_1pl));\n",
    "print(\"Number of posterior samples:\", len(samples_1pl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = corner.corner(samples_1pl, labels=[\"p\",\"K\",\"e\",\"$\\omega$\",\"M\"],\\\n",
    "                   truths=MLE_1pl, label_kwargs={\"fontsize\":20})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section2.2'></a>\n",
    "### <center>2.2. Two-planet model </center>\n",
    "\n",
    "Next, we will follow the same procedure as the one-planet model but with two-planet model. We will first compute a MLE which will hopefully look very similar to our input conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "system_2pl = params_true_chained\n",
    "MLE_2pl = list(op.minimize( nll, system_2pl, args=obs, bounds=bounds(system_2pl))['x'])\n",
    "\n",
    "print(\"MLE for two-planet model [p, K, e, w, M, ...]:\")\n",
    "print(MLE_2pl)\n",
    "print(\"\")\n",
    "print(\"Actual input model       [p, K, e, w, M, ...]:\")\n",
    "print(system_2pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we perform MCMC but on a two-planet model. This will take considerably longer with an extra body and more Markov chains ($\\tt{emcee}$ asserts that the number of walkers >= 2 * number of dimensions). I clocked this run to take roughly 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nDim_2pl = len(MLE_2pl)\n",
    "nWalkers_2pl = 2*nDim_2pl\n",
    "init_2pl = [ create_init(MLE_2pl) for i in range(nWalkers_2pl) ]\n",
    "\n",
    "sampler_2pl = emcee.EnsembleSampler(nWalkers_2pl, nDim_2pl, lnpost, args=[obs])\n",
    "sampler_2pl.run_mcmc(init_2pl, 11000, thin=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_2pl = sampler_2pl.chain[:, 25:, :].reshape((-1, nDim_2pl));\n",
    "print(\"Number of posterior samples:\", len(samples_2pl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = corner.corner(samples_2pl, labels=[\"p1\",\"K1\",\"e1\",\"$\\omega$1\",\"M1\",\"p2\",\"K2\",\"e2\",\"$\\omega$2\",\"M2\"],\\\n",
    "                   truths=MLE_2pl, label_kwargs={\"fontsize\":20})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "<a id='section3'></a>\n",
    "## <center>3. Do the FML/Bayes Factor calculation </center>\n",
    "\n",
    "With out posterior samples for the one-planet model ($\\tt{samples\\_1pl}$) and two-planet model ($\\tt{samples\\_2pl}$), we can now estimate fully marginalized likelihoods using a modified importance sampling algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>3.1. Read in posterior samples and set parameterization for importance sampling. </center>\n",
    "\n",
    "First, we read in MCMC samples from the $\\tt{emcee}$ runs above and set up a parameterization for the importance sampling. We read in orbital period ($p$), RV half amplitude ($K$), eccentricity ($e$), argument of pericenter ($\\omega$), and mean anomaly ($M$). We go from $\\{p, K, e, \\omega, M\\}$ (pKewM for short) to $\\{p, K, e\\sin\\omega, e\\cos\\omega, \\omega+M\\}$ (pKhkl for short) for the importance sampling then back to $\\{p, K, e, \\omega, M\\}$ for the model evaluation. The \"dict_keys\" method creates a hardwired list of dictionary keys for both parameterizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nPlanets = 1\n",
    "\n",
    "param_keys, param_IS_keys = create.dict_keys(nPlanets)\n",
    "print(\"Parameterization for MCMC and REBOUND input:\", param_keys)\n",
    "print(\"Parameterization for importance sampling:\", param_IS_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "\n",
    "\n",
    "From here, we use \"posterior_samples_from_file\" to convert the input posterior file to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postSamp, nPostSamples = create.posterior_samples_from_emcee(samples_1pl, param_keys)\n",
    "print(\"Posterior samples of eccentricity of planet 1 :\", postSamp['e1'][0:5])\n",
    "print(\"Posterior samples of argperi of planet 1      :\", postSamp['w1'][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "\n",
    "\n",
    "To deal with boundary issues, we first transform $\\{e, \\omega, M\\}$ to $\\{e\\sin{\\omega}, e\\cos{\\omega}, \\omega+M\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postSamp_pKhkl = compute.pKewM_to_pKhkl(postSamp, param_IS_keys, nPlanets)\n",
    "print(\"Posterior samples of esinw of planet 1 :\", postSamp_pKhkl['esinw1'][0:5])\n",
    "print(\"Posterior samples of ecosw of planet 1 :\", postSamp_pKhkl['ecosw1'][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure everything works, let's convert $\\tt{postSamp\\_pKhkl}$ back to $\\{e, \\omega, M\\}$ space to see if it's the same as $\\tt{postSamp}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postSamp_test = compute.pKhkl_to_pKewM(postSamp_pKhkl, param_keys, nPlanets)\n",
    "print(\"Posterior samples of eccentricity of planet 1 :\", postSamp_test['e1'][0:5])\n",
    "print(\"Posterior samples of argperi of planet 1      :\", postSamp_test['w1'][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <center>3.2. Do the importance sampling.</center>\n",
    "\n",
    "Formally, we are going to compute\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\vec{d}) \\approx \\frac{1}{N \\times f_{MCMC}} \\sum_{i=1}^{N} \\frac{p(\\vec{\\theta})p(\\vec{d}|\\vec{\\theta})}{g(\\vec{\\theta})}\n",
    "\\end{equation}\n",
    "where $p(\\vec{\\theta})p(\\vec{d}|\\vec{\\theta})$ is essentially the $\\tt{lnpost}$ function defined near the beginning of Section 2, $g(\\theta)$ is the importance sampling function used to approximate $p(\\vec{\\theta})p(\\vec{d}|\\vec{\\theta})$, $N$ is the number of importance samples, and $f_{MCMC}$ is the fraction of MCMC samples to reside in $g(\\theta)$. More on the latter later.\n",
    "\n",
    "\n",
    "The magic behind importance sampling is choosing a suitable distribution $g(\\vec{\\theta})$ as an approximation for $p(\\vec{\\theta})p(\\vec{d}|\\vec{\\theta})$. Here, we choose a multivariable normal $\\mathcal{N}(\\mu,\\Sigma)$ where $\\mu$ is the mean vector and $\\Sigma$ is the covariance matrix of our set of model parameters estimated from the posterior sample.\n",
    "\n",
    "The \"matrix info\" function computes the $\\vec{\\mu}$, $\\vec{\\Sigma}$ and its associated Cholesky decomposition, and $\\log(\\det(\\Sigma))$, which together allow us to perform the importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mediansG, covMatrixG, choleskyDecomp, logDetSigmaG = compute.matrix_info(postSamp_pKhkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Typically with importance sampling, you want to pick a $g(\\vec{\\theta})$ that is a bit wider than $p(\\vec{\\theta})p\\vec{d}|\\vec{\\theta})$ in order to sample from the lower probability tails. After all, we are numerically integrating $p(\\vec{\\theta})p(\\vec{d}|\\vec{\\theta})$ and need sufficient coverage over $\\vec{\\theta}$ to obtain an accurate estimate of $p(\\vec{d}|\\mathcal{M})$.\n",
    "\n",
    "To get around this, we will draw from a truncated multivariate normal and supplement the lost information with the MCMC samples. The truncation happens at $x$-$\\sigma$ away in each parameter. So $x=2$ means we draw from a truncated normal between -2-$\\sigma$ and 2-$\\sigma$ of each parameter. Studies done by [Peng-Cheng Guo 2012](http://adsabs.harvard.edu/abs/2012PhDT.......233G) and [Nelson et al. 2016](http://adsabs.harvard.edu/abs/2016MNRAS.455.2484N) show x=1.5-2.0 work pretty well for 1-4 planet models.\n",
    "\n",
    "We'll choose x=1.5 for this tutorial. For brevity, we'll only draw 1000 importance samples. It's not clear if this is sufficient for a robust estimate, but we'll do better after this demonstration.\n",
    "\n",
    "### <center> Here is the importance sampling in action! </center> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The modules below allow us to create a progress bar for our for loops\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "scale = 1.5\n",
    "nImportSamps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "from math import erf\n",
    "\n",
    "nParams = len(param_keys)\n",
    "random_values = [ truncnorm.rvs(-scale, scale, size=nParams) for i in range(nImportSamps) ]\n",
    "\n",
    "samples = [ [] for i in range(nImportSamps) ]\n",
    "g_samples = [ [] for i in range(nImportSamps) ]\n",
    "loggs = [ 0. for i in range(nImportSamps) ]\n",
    "\n",
    "# visualizes a progress bar\n",
    "f = FloatProgress(min=0, max=nImportSamps)\n",
    "display(f)\n",
    "\n",
    "for x in range(nImportSamps):\n",
    "    dispersion = np.dot( choleskyDecomp, np.transpose(random_values[x]) )\n",
    "    samples[x] = mediansG + dispersion\n",
    "    g_samples[x] = list(samples[x])\n",
    "\n",
    "    logg = -0.5 * (nParams*np.log(2.*np.pi) + logDetSigmaG + \\\n",
    "                    np.dot( np.transpose(np.subtract(samples[x],mediansG)), \\\n",
    "                    np.linalg.solve(covMatrixG, np.subtract(samples[x],mediansG) ) ) ) - \\\n",
    "                    nParams*np.log(erf(scale/np.sqrt(2.)))\n",
    "    loggs[x] = logg\n",
    "    f.value = x\n",
    "\n",
    "print(\"Done drawing importance samples.\")\n",
    "    \n",
    "g_samples_T = np.transpose(g_samples)\n",
    "importSamp_dict = OrderedDict()\n",
    "for i, item in enumerate(g_samples_T):\n",
    "    importSamp_dict[param_IS_keys[i]] = item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "\n",
    "Now we can visually compare the posterior samples to the importance samples in a subspace of two parameters. Assign \"a\" and \"b\" to any pair of parameter keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Possible parameters to plot:\", param_IS_keys)\n",
    "\n",
    "a, b = param_IS_keys[0], param_IS_keys[-1]\n",
    "\n",
    "plt.plot(postSamp_pKhkl[a], postSamp_pKhkl[b], 'o', color='blue')\n",
    "plt.plot(importSamp_dict[a], importSamp_dict[b], 'x', color='red')\n",
    "plt.legend([\"Posterior samples\",\"Importance samples\"])\n",
    "plt.xlabel(a, fontsize=16)\n",
    "plt.ylabel(b, fontsize=16)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 3.3. Evaluate importance samples at $p(\\vec{d}|\\vec{\\theta})p(\\vec{\\theta})$. </center>\n",
    "\n",
    "After transforming back into pKewM space..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importSamp_pKhkl_dict = compute.pKhkl_to_pKewM(importSamp_dict, param_keys, nPlanets)\n",
    "importSamp_pKewM = np.transpose([ vals for key, vals in importSamp_pKhkl_dict.items() ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "...we feed our importance samples into $\\tt{lnpost}$, defined above when doing the $\\tt{emcee}$ runs. Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = FloatProgress(min=0, max=nImportSamps)\n",
    "display(f)\n",
    "\n",
    "logPosteriors = np.array([ np.nan for i in range(nImportSamps) ])\n",
    "for i in range(nImportSamps): \n",
    "    logPosteriors[i] = lnpost(importSamp_pKewM[i], obs)\n",
    "    f.value = i\n",
    "    \n",
    "print(\"Done evaluating lnpost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to sum over $p(\\vec{\\theta_i})p(\\vec{d}|\\vec{\\theta_i})/g(\\vec{\\theta_i})$ then divide by the number of importance samples. These numbers are probably ridiculously small, so we'll operate on $\\log[p(\\vec{\\theta_i})p(\\vec{d}|\\vec{\\theta_i})]$ and $\\log[g(\\vec{\\theta_i})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logSum = -(9.**99.)\n",
    "\n",
    "for i in range(nImportSamps):    \n",
    "    diff = logPosteriors[i] - loggs[i]\n",
    "    logSum = np.logaddexp(logSum, diff)\n",
    "    \n",
    "    if i%100==1:\n",
    "        print(\"logAvg estimate after\", i, \"samples:\", logSum - np.log(i))\n",
    "\n",
    "logAvg = logSum - np.log(nImportSamps)\n",
    "print(logAvg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we'll supplement the information lost from when we drew from a truncated normal. Here, we compute the fraction of MCMC samples that fell within our hypercube $\\tt{f_{MCMC}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_MCMC = 0.\n",
    "\n",
    "postSamp_wo_keys = []\n",
    "for key in postSamp_pKhkl:\n",
    "    postSamp_wo_keys.append(postSamp_pKhkl[key])\n",
    "    \n",
    "postSamp_wo_keys = np.transpose(np.array(postSamp_wo_keys))\n",
    "diff = postSamp_wo_keys-mediansG\n",
    "\n",
    "for j in range(nPostSamples):\n",
    "\n",
    "    z = np.linalg.solve(choleskyDecomp, diff[j])\n",
    "\n",
    "    if all([abs(k)<=scale for k in z]):\n",
    "        f_MCMC += 1.\n",
    "    else:\n",
    "        f_MCMC += 0.\n",
    "        \n",
    "f_MCMC = f_MCMC/nPostSamples\n",
    "logFML = logAvg - np.log(f_MCMC)\n",
    "print(\"f_MCMC:\", f_MCMC)\n",
    "print(\"logFML:\", logFML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We've now computed the fully marginalized likelihood for the one-planet model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section3.4'></a>\n",
    "### <center> 3.4. The real deal: computing Bayes factor for one- and two-planet models. </center>\n",
    "Let's gather all these functions together into a new module called $\\tt{FML.py}$. Now we have a general algorithm to compute Bayes factors for a given input model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import FML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FML_1pl = FML.computeFML(samples_1pl, obs=obs, nPlanets=1, nImportSamps=10000, scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FML_2pl = FML.computeFML(samples_2pl, obs=obs, nPlanets=2, nImportSamps=10000, scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The ratio of the FMLs (or difference of log[FML]s) gives a Bayes factor (log[Bayes factor]). We can compare our results to other model comparison choices such as the Bayesian or Akaike Information Criterion (BIC and AIC respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AIC(MLE, obs):\n",
    "    return 2. * len(MLE) - 2*lnlike(MLE, obs)\n",
    "\n",
    "def BIC(MLE, obs):\n",
    "    return np.log(len(obs.rv)) * len(MLE) - 2*lnlike(MLE, obs)\n",
    "\n",
    "\n",
    "BIC_1pl, BIC_2pl = BIC(MLE_1pl, obs), BIC(MLE_2pl, obs)\n",
    "AIC_1pl, AIC_2pl = AIC(MLE_1pl, obs), AIC(MLE_2pl, obs)\n",
    "\n",
    "print(\"log Bayes Factor :\", FML_2pl.logFML - FML_1pl.logFML)\n",
    "print(\"delta BIC        :\", BIC_1pl - BIC_2pl)\n",
    "print(\"delta AIC        :\", AIC_1pl - AIC_2pl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
